<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Neural Procedural Reconstruction for Residential Buildings</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="../css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="../css/cayman.css">
    <link rel="stylesheet" href="../css/erik.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Neural Procedural Reconstruction for Residential Buildings</h1>
      <h2 class="project-tagline"><a class="site-link" href="http://www.cse.wustl.edu/~zengh">Huayi Zeng</a>, Charlie Wu and <a class="site-link" href="http://www.cse.wustl.edu/~furukawa">Yasutaka Furukawa</a></h2>
      <h2> Accepted by ECCV 2018</a></h2>
    </section>

    <section class="main-content">
      <h2>Abstract</h2>
      <p>This paper proposes a novel 3D reconstruction approach, dubbed Neural Procedural Reconstruction (NPR). NPR infers a sequence of shape grammar rule applications and reconstructs CAD-quality models with procedural structure from 3D points. While most existing methods rely on low-level geometry analysis to extract primitive structures, our approach conducts global analysis of entire building structures by deep neural networks (DNNs), enabling the reconstruction even from incomplete and sparse input data. We demonstrate the proposed system for residential buildings with aerial LiDAR as the input. Our 3D models boast compact geometry and semantically segmented architectural components. Qualitative and quantitative evaluations on hundreds of houses demonstrate that the proposed approach makes signicant improvements over the existing state-of-the-art.</p>

        <h2>Paper</h2>
        <p><a href="eccv2018_procedural.pdf">Main paper</a></p>
<!--         <h2>Video</h2>
        <iframe width="560" height="315" src="https://youtu.be/9yyFlE4lWUo" frameborder="0" allowfullscreen></iframe>
 -->        
        <h2>Code</h2>
        <p><a href="https://drive.google.com/file/d/1pW4-TgpfxXGY0_DmTG-RcWK6nkPt8Bvf/view?usp=sharing">Code</a> (still updating)</p>
        <h2>Raw LiDAR Data</h2>
        <p>Can be downloaded <a href="https://environment.data.gov.uk/ds/survey">here</a></p>        
        <h2>Pre-processed Data and Annotation</h2>
        <p><a href="https://drive.google.com/file/d/1lgfYtXpLLL-uMz3dMfJlf3Gk7ElsPXRf/view">Rotated Data</a></p>
        <p><a href="https://drive.google.com/file/d/1QmkzZrL0GnFC3pobGqbXR1OXsaRNXTiI/view">Rotated Annotations</a></p>
        <h2>Acknowledgement</h2>
        <p>This research is partially supported by National Science Foundation under grant
            IIS 1540012 and IIS 1618685, and Google Faculty Research Award. We thank
            Nvidia for a generous GPU donation.</p>

        <footer class="site-footer">
          <span class="site-footer-owner"> <a href="http://www.cse.wustl.edu/~zengh">Huayi Zeng@2018</a></span>
          <span class="site-footer-credits"> This page adopts <a href="https://github.com/jasonlong/cayman-theme">Cayman</a> Theme.</span>
        </footer>

      </section>
    </body>
    </html>
